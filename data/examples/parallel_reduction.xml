<?xml version="1.0" encoding="UTF-8"?>
<example>
    <name>Parallel Reduction</name>
    <category>Basic</category>
    <sourceFile>src/03_parallel_reduction/reduction.cu</sourceFile>
    <description>
        <title>Parallel Reduction üå≥‚ûï - From Millions to One with Logarithmic Efficiency</title>
        <overview>
            This example demonstrates parallel reduction algorithms that efficiently combine millions of values 
            into a single result (sum, max, min, etc.) using tree-based approaches. Like a tournament 
            where participants are eliminated in pairs until one winner remains, parallel reduction 
            organizes thousands of GPU threads in a hierarchical structure to achieve maximum efficiency.
        </overview>
        <analogy>
            <strong>Think of it like this:</strong> Imagine adding a million numbers by organizing a tournament. 
            In round 1, pairs of people add their two numbers. In round 2, pairs add their results from round 1. 
            This continues until only one person remains with the final sum. Instead of taking a million steps 
            sequentially, you only need about 20 rounds (log‚ÇÇ of a million) - this is the power of parallel reduction!
        </analogy>
        <features>
            <feature><strong>Tree-Based Reduction:</strong> Logarithmic time complexity instead of linear</feature>
            <feature><strong>Warp-Level Primitives:</strong> Hardware-accelerated shuffle operations for ultimate speed</feature>
            <feature><strong>Multiple Algorithms:</strong> Compare different reduction strategies and their performance</feature>
            <feature><strong>Atomic Operations:</strong> Safe inter-block communication for global results</feature>
            <feature><strong>Memory Optimization:</strong> Shared memory usage to minimize global memory access</feature>
        </features>
        <concepts>
            <concept>
                <title>üå≤ Tree Reduction Algorithm</title>
                <description>Hierarchical pairing reduces O(n) to O(log n) complexity - the fundamental parallel pattern</description>
            </concept>
            <concept>
                <title>üîÄ Warp Shuffle Operations</title>
                <description>__shfl_down_sync() - hardware-accelerated data exchange within a 32-thread warp</description>
            </concept>
            <concept>
                <title>‚öõÔ∏è Atomic Operations</title>
                <description>atomicAdd(), atomicMax() - thread-safe operations for combining results across blocks</description>
            </concept>
            <concept>
                <title>üß± Block-Level Reduction</title>
                <description>Shared memory coordination among threads in the same block for intermediate results</description>
            </concept>
            <concept>
                <title>üîÑ Warp-Level Primitives</title>
                <description>Cooperative groups and modern CUDA primitives for maximum hardware efficiency</description>
            </concept>
        </concepts>
        <applications>
            <application>
                <title>üìä Big Data Analytics</title>
                <description>Computing sums, averages, and statistics across massive datasets in real-time</description>
            </application>
            <application>
                <title>üß† Machine Learning</title>
                <description>Loss computation, gradient norms, and metric calculations during training</description>
            </application>
            <application>
                <title>üî¨ Scientific Computing</title>
                <description>Numerical integration, norm calculations, and convergence criteria in simulations</description>
            </application>
            <application>
                <title>üíπ Financial Analysis</title>
                <description>Portfolio risk calculations, option pricing models, and market data aggregation</description>
            </application>
            <application>
                <title>üéÆ Game Development</title>
                <description>Physics simulations, collision detection, and real-time performance monitoring</description>
            </application>
        </applications>
        <performance>
            <consideration>Warp divergence - ensure all threads in a warp participate in reduction steps</consideration>
            <consideration>Memory bank conflicts - organize shared memory access to avoid conflicts</consideration>
            <consideration>Occupancy vs shared memory - balance thread count with memory usage per block</consideration>
            <consideration>Atomic contention - minimize atomic operations at global scope for best performance</consideration>
            <consideration>Data type considerations - use appropriate precision to balance speed and accuracy</consideration>
        </performance>
        <importance>
            <why>
                Parallel reduction is one of the most fundamental parallel algorithms. It appears everywhere: 
                computing averages, finding maximums, calculating norms, and determining convergence. 
                Master this pattern, and you understand the key to efficiently combining distributed 
                computations - a skill essential for virtually all parallel programming challenges.
            </why>
            <performance>
                A well-optimized parallel reduction can process billions of numbers in milliseconds, 
                achieving 1000x speedup over sequential CPU code. The logarithmic complexity means 
                that doubling your data size barely increases computation time - this scalability 
                is what makes "Big Data" analytics practical on modern GPUs.
            </performance>
        </importance>
    </description>
</example>
